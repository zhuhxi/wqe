import torch
import torch.nn as nn
import torch.nn.functional as F

# ============================================================
# 1. Global Response Normalization (ConvNeXt V2)
#    Paper: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"
#    X_out = X + gamma * (X * N(G(X))) + beta
#    G(X) : L2 norm over spatial dims, then normalized across channels
# ============================================================

class GRN(nn.Module):
    """
    Global Response Normalization (GRN) layer.
    é€‚ç”¨è¾“å…¥: (B, C, H, W)ï¼Œä¸æ”¹å˜é€šé“æ•°å’Œåˆ†è¾¨ç‡ã€‚

    dim: é€šé“æ•° C
    """

    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.zeros(1, dim, 1, 1))
        self.beta = nn.Parameter(torch.zeros(1, dim, 1, 1))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, C, H, W)
        # 1) ç©ºé—´ç»´åº¦ä¸Šçš„ L2 èŒƒæ•°  G(x) âˆˆ (B, C, 1, 1)
        Gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)

        # 2) åœ¨é€šé“ç»´åº¦ä¸Šåšå½’ä¸€åŒ–ï¼Œå½¢æˆé€šé“é—´ç«äº‰
        #    N(Gx)_c = Gx_c / (mean_c' Gx_c' + eps)
        Gx_mean = Gx.mean(dim=1, keepdim=True)
        Nx = Gx / (Gx_mean + self.eps)

        # 3) æ ¡å‡†åŸå§‹å“åº”å¹¶åŠ  residual
        return x + self.gamma * (x * Nx) + self.beta


# ============================================================
# 2. Multi-scale Depthwise Conv Block (å‚è€ƒ MRLPFNet & EMCAD)
#    - å¤šä¸ªä¸åŒ kernel size çš„ depthwise conv
#    - èšåˆååšä¸€æ¬¡ pointwise conv
#    - ä¿æŒè¾“å…¥è¾“å‡º shape ä¸€è‡´: (B, C, H, W) -> (B, C, H, W)
# ============================================================

class MultiScaleDWConvBlock(nn.Module):
    """
    Multi-scale Depthwise Convolution Block.

    å‚è€ƒ:
      - "Multi-scale Residual Low-Pass Filter Network for Image Deblurring" (ICCV 2023)
      - "EMCAD: Efficient Multi-scale Convolutional Attention Decoding for Medical Image Segmentation" (CVPR 2024)
    ä½†å®ç°æ˜¯ç®€åŒ–ç‰ˆï¼Œæ–¹ä¾¿ä½ å½“é€šç”¨å°æ¨¡å—æ’åœ¨ UNet / EcNet é‡Œã€‚

    Args:
        dim: è¾“å…¥/è¾“å‡ºé€šé“æ•° C
        kernel_sizes: å¤šå°ºåº¦ depthwise å·ç§¯çš„ kernel size åˆ—è¡¨
        use_grn: æ˜¯å¦åœ¨ block å†…éƒ¨å ä¸€å±‚ GRN
    """

    def __init__(
        self,
        dim: int,
        kernel_sizes=(3, 5, 7),
        use_grn: bool = True,
        act_layer=nn.GELU,
    ):
        super().__init__()
        self.dim = dim

        # depthwise conv åˆ†æ”¯
        dw_convs = []
        for k in kernel_sizes:
            padding = k // 2
            dw_convs.append(
                nn.Sequential(
                    nn.Conv2d(dim, dim, kernel_size=k, padding=padding, groups=dim, bias=False),
                    nn.BatchNorm2d(dim),
                    act_layer(),
                )
            )
        self.dw_convs = nn.ModuleList(dw_convs)

        # èšåˆåçš„ pointwise conv
        self.pw = nn.Conv2d(dim, dim, kernel_size=1, bias=False)
        self.pw_bn = nn.BatchNorm2d(dim)

        self.act = act_layer()
        self.use_grn = use_grn
        if use_grn:
            self.grn = GRN(dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, C, H, W)
        residual = x

        # å¤šå°ºåº¦ depthwise å·ç§¯ï¼Œç»“æœæ±‚å’Œ
        out = 0
        for branch in self.dw_convs:
            out = out + branch(x)
        out = out / len(self.dw_convs)

        # pointwise èåˆ
        out = self.pw_bn(self.pw(out))

        # å¯é€‰ GRN
        if self.use_grn:
            out = self.grn(out)

        # æ®‹å·® + æ¿€æ´»
        out = out + residual
        out = self.act(out)
        return out


# ============================================================
# 3. é€šç”¨æµ‹è¯•è„šæœ¬
#    - è·Ÿä½ å‘çš„ EcNet å°è„šæœ¬åŒä¸€ä¸ªé£æ ¼
#    - Forward shape æ£€æŸ¥
#    - NNI ç»Ÿè®¡ FLOPs / Params
# ============================================================

def build_test_module(name: str):
    """
    æ ¹æ®åå­—æ„é€ ä¸€ä¸ªå¾…æµ‹è¯•çš„â€œå°æ¨¡å—â€ä»¥åŠå¯¹åº”çš„è¾“å…¥ shapeã€‚
    ä½ åé¢æŠ æ–°æ¨¡å—ï¼Œå°±åœ¨è¿™é‡ŒåŠ ä¸€ä¸ª elif åˆ†æ”¯å³å¯ã€‚
    """
    if name.lower() == "grn":
        dim = 16
        module = GRN(dim=dim)
        input_shape = (1, dim, 16, 16)

    elif name.lower() == "msdw":
        dim = 16
        module = MultiScaleDWConvBlock(dim=dim)
        input_shape = (1, dim, 16, 16)

    else:
        raise ValueError(f"Unknown module name: {name}")

    return module, input_shape


if __name__ == "__main__":
    # ä½ å¯ä»¥åœ¨è¿™é‡Œåˆ‡æ¢è¦æµ‹è¯•çš„æ¨¡å—åå­—: "grn" æˆ– "msdw"
    module_name = "msdw"  # æ”¹æˆ "grn" å°±èƒ½æµ‹ GRN

    # ---- æ„é€ æ¨¡å— & éšæœºè¾“å…¥ ----
    model, shape = build_test_module(module_name)
    b, c, h, w = shape
    x = torch.rand(b, c, h, w)

    print(f"ğŸ”§ Testing module: {module_name}")
    print(f"   Input  shape: {tuple(x.shape)}")

    # --- Shape æµ‹è¯• ---
    try:
        out = model(x)
        print(f"âœ… Forward Pass Success: {x.shape} â†’ {out.shape}")
    except Exception as e:
        print(f"âŒ Forward Failed: {e}")

    # --- FLOPs å’Œ å‚æ•°ç»Ÿè®¡ ---
    try:
        from nni.compression.utils.counter import count_flops_params

        flops, params, _ = count_flops_params(model, x=(x,))
        print(f"ğŸ“Š FLOPs:  {flops / 1e6:.2f} MFLOPs")
        print(f"ğŸ“¦ Params: {params / 1e6:.2f} M")
    except ImportError:
        print("âš ï¸ NNI not installed. Run: pip install nni")
    except Exception as e:
        print(f"âš ï¸ FLOPs/Params counting failed: {e}")
