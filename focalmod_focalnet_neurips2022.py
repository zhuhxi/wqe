import torch
import torch.nn as nn
import torch.nn.functional as F


# ============================================================
# Focal Modulation Module
#   Paper: "Focal Modulation Networks", NeurIPS 2022
#   Official repo: https://github.com/microsoft/FocalNet
#
#   è¿™æ˜¯åŸºäºè®ºæ–‡å…¬å¼çš„ç®€åŒ–å®ç°ï¼Œä¿ç•™äº†æ ¸å¿ƒç»“æ„ï¼š
#   - å¤šå±‚ depthwise conv åšåˆ†å±‚è¯­å¢ƒåŒ– (hierarchical context)
#   - gating èšåˆå¤šå°ºåº¦ & å…¨å±€ä¸Šä¸‹æ–‡
#   - çº¿æ€§æ˜ å°„å¾—åˆ°è°ƒåˆ¶å™¨ï¼ŒæŒ‰å…ƒç´ ä¹˜åˆ° query ä¸Š
#
#   è¾“å…¥ / è¾“å‡º: (B, C, H, W) -> (B, C, H, W)
# ============================================================


class FocalModulation(nn.Module):
    """
    Focal Modulation module (simplified, PyTorch, NCHW).

    Args:
        dim: é€šé“æ•° C
        focal_levels: å¤šå°ºåº¦å±‚æ•° Lï¼ˆè®ºæ–‡ä¸­ä¸€èˆ¬æ˜¯ 2~4ï¼‰
        kernel_sizes: æ¯ä¸€å±‚ depthwise conv çš„ kernel size åˆ—è¡¨ï¼Œé•¿åº¦è¦ç­‰äº focal_levels
        use_post_ln: æ˜¯å¦åœ¨è¾“å‡ºååŠ ä¸€å±‚ LayerNorm (channels_last)ï¼Œæ–¹ä¾¿æ’åˆ° ViT ç±»ç»“æ„
        act_layer: æ¿€æ´»å‡½æ•°
    """

    def __init__(
        self,
        dim: int,
        focal_levels: int = 3,
        kernel_sizes=None,
        use_post_ln: bool = False,
        act_layer=nn.GELU,
    ):
        super().__init__()
        self.dim = dim
        self.focal_levels = focal_levels
        if kernel_sizes is None:
            # é»˜è®¤ä»å°åˆ°å¤§å‡ ä¸ª kernel
            kernel_sizes = [3, 5, 7][:focal_levels]
        assert len(kernel_sizes) == focal_levels, "kernel_sizes length must equal focal_levels"

        # 1) f_z: è¾“å…¥æŠ•å½±åˆ° Z^0
        self.proj_in = nn.Conv2d(dim, dim, kernel_size=1, bias=True)

        # 2) L å±‚ depthwise conv + æ¿€æ´»: Z^ell = GELU(DWConv(Z^{ell-1}))
        self.dw_convs = nn.ModuleList()
        for k in kernel_sizes:
            padding = k // 2
            self.dw_convs.append(
                nn.Sequential(
                    nn.Conv2d(dim, dim, kernel_size=k, padding=padding, groups=dim, bias=False),
                    nn.BatchNorm2d(dim),
                    act_layer(),
                )
            )

        # 3) gating: ä»åŸå§‹ X äº§ç”Ÿ (L+1) ä¸ª gating map
        #    å½¢çŠ¶: (B, L+1, H, W)ï¼Œæ¯ä¸ªé€šé“å¯¹åº”ä¸€ä¸ªå°ºåº¦çš„ gate
        self.gating = nn.Conv2d(dim, focal_levels + 1, kernel_size=1, bias=True)

        # 4) h: æŠŠèšåˆåçš„ Z_out -> è°ƒåˆ¶å™¨ M (åŒé€šé“æ•°)
        self.modulator_proj = nn.Conv2d(dim, dim, kernel_size=1, bias=True)

        # 5) q: query projection
        self.query_proj = nn.Conv2d(dim, dim, kernel_size=1, bias=True)

        # å¯é€‰: è¾“å‡ºååšä¸€æ¬¡ LayerNorm (channels_last)
        self.use_post_ln = use_post_ln
        if use_post_ln:
            self.ln = nn.LayerNorm(dim)

        self.act = act_layer()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, C, H, W)
        return: (B, C, H, W)
        """
        B, C, H, W = x.shape

        # ---- 1) æŠ•å½±å¾—åˆ° Z^0 ----
        z = self.proj_in(x)  # (B, C, H, W)

        # ---- 2) åˆ†å±‚è¯­å¢ƒåŒ– Z^ell ----
        # ä¿å­˜æ¯ä¸€å±‚çš„ç‰¹å¾ï¼Œæœ€åä¸€å±‚å†æ‹¿å»åš global pooling
        zs = []
        cur = z
        for dw in self.dw_convs:
            cur = dw(cur)
            zs.append(cur)  # æ¯ä¸ªéƒ½æ˜¯ (B, C, H, W)

        # ---- 3) å…¨å±€ä¸Šä¸‹æ–‡ Z^{L+1} ----
        # å¯¹æœ€åä¸€å±‚åš GAPï¼Œå† broadcast å› H x W
        z_global = cur.mean(dim=(2, 3), keepdim=True)  # (B, C, 1, 1)
        z_global = z_global.expand(-1, -1, H, W)       # (B, C, H, W)

        # ä¸€å…± L+1 ä¸ªå°ºåº¦ç‰¹å¾
        zs.append(z_global)  # len(zs) = L + 1

        # ---- 4) gating èšåˆ ----
        gate_logits = self.gating(x)  # (B, L+1, H, W)
        # å¯ä»¥ç”¨ sigmoid ä¿è¯åœ¨ 0~1 èŒƒå›´ï¼Œä¹Ÿå¯ä»¥ softmaxï¼Œè¿™é‡Œç”¨ sigmoid è¶³å¤Ÿç®€å•
        gates = torch.sigmoid(gate_logits)

        # å¯¹æ¯ä¸ªå°ºåº¦åšåŠ æƒæ±‚å’Œ
        # Z_out = sum_{ell}( G^ell * Z^ell )
        z_out = 0.0
        for level_idx in range(self.focal_levels + 1):
            g_l = gates[:, level_idx : level_idx + 1, :, :]  # (B, 1, H, W)
            z_l = zs[level_idx]                              # (B, C, H, W)
            z_out = z_out + g_l * z_l                        # broadcast on channels

        # ---- 5) é€šé“ç»´åº¦ä¸Šçš„è°ƒåˆ¶å™¨ M ----
        m = self.modulator_proj(z_out)  # (B, C, H, W)
        m = self.act(m)

        # ---- 6) query projection + å…ƒç´ çº§è°ƒåˆ¶ ----
        q = self.query_proj(x)          # (B, C, H, W)
        y = q * m                       # Focal Modulation: q(x) âŠ™ M(x, X)

        # å¯é€‰: è¾“å‡ºå±‚åš LayerNorm (channels_last)
        if self.use_post_ln:
            y_perm = y.permute(0, 2, 3, 1)   # (B, H, W, C)
            y_perm = self.ln(y_perm)
            y = y_perm.permute(0, 3, 1, 2)   # å›åˆ° (B, C, H, W)

        return y


# ============================================================
# ä¸€ä¸ªç®€å•çš„ Block å°è£…: norm + FocalModulation + residual
#   æ–¹ä¾¿ä½ ç›´æ¥åœ¨ backbone / UNet é‡Œæ›¿æ¢åŸæ¥çš„ self-attention block
# ============================================================

class FocalModulationBlock(nn.Module):
    """
    æ ‡å‡† Block: (Norm -> FocalModulation -> Dropout) + Residual

    è¾“å…¥ / è¾“å‡º: (B, C, H, W)
    """

    def __init__(
        self,
        dim: int,
        focal_levels: int = 3,
        kernel_sizes=None,
        drop: float = 0.0,
        use_post_ln: bool = False,
    ):
        super().__init__()
        self.norm = nn.BatchNorm2d(dim)
        self.focal = FocalModulation(
            dim=dim,
            focal_levels=focal_levels,
            kernel_sizes=kernel_sizes,
            use_post_ln=use_post_ln,
        )
        self.drop = nn.Dropout2d(drop) if drop > 0.0 else nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = x
        out = self.norm(x)
        out = self.focal(out)
        out = self.drop(out)
        out = out + residual
        return out


# ============================================================
# æµ‹è¯•è„šæœ¬ (å’Œä½  EcNet å°è„šæœ¬åŒé£æ ¼)
#   - Forward shape æµ‹è¯•
#   - NNI ç»Ÿè®¡ FLOPs / Params (å¯é€‰)
# ============================================================

if __name__ == "__main__":
    # ä½ å¯ä»¥åœ¨è¿™é‡Œåˆ‡æ¢æµ‹è¯• FocalModulation æˆ– FocalModulationBlock
    TEST_BLOCK = "focal_mod"  # "focal_mod" æˆ– "block"

    b = 1
    input_size = 16
    dim = 16

    x = torch.rand(b, dim, input_size, input_size)  # è¾“å…¥: (B, C, H, W)

    if TEST_BLOCK == "focal_mod":
        model = FocalModulation(dim=dim, focal_levels=3)
    else:
        model = FocalModulationBlock(dim=dim, focal_levels=3, drop=0.0)

    print(f"ğŸ”§ Testing: {TEST_BLOCK}")
    print(f"   Input  shape: {tuple(x.shape)}")

    # --- Shape æµ‹è¯• ---
    try:
        out = model(x)
        print(f"âœ… Forward Pass Success: {x.shape} â†’ {out.shape}")
    except Exception as e:
        print(f"âŒ Forward Failed: {e}")

    # --- FLOPs å’Œ å‚æ•°ç»Ÿè®¡ ---
    try:
        from nni.compression.utils.counter import count_flops_params

        flops, params, _ = count_flops_params(model, x=(x,))
        print(f"ğŸ“Š FLOPs:  {flops / 1e6:.2f} MFLOPs | Params: {params / 1e6:.4f} M")
    except ImportError:
        print("âš ï¸ NNI not installed. Run: pip install nni")
    except Exception as e:
        print(f"âš ï¸ FLOPs/Params counting failed: {e}")
