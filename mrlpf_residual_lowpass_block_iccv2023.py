import torch
import torch.nn as nn
import torch.nn.functional as F


# ============================================================
# Multi-scale Residual Low-Pass Filter Block (MRLPF Block)
#
# Paper:
#   "Multi-Scale Residual Low-Pass Filter Network for Image Deblurring"
#   (ICCV 2023, Dong et al.)
#
# å¯¹åº”è®ºæ–‡ä¸­çš„ RLPF æ¨¡å— (Eqs. (3), (4), (5)):
#
#   F = Y + R(Y) + F_freq(Y)                         (3)
#   H_i = sum_j S_ij f_j     (Self-Attention as LPF) (4)
#   E = F + D(H)                                    (5)
#
# è¿™é‡Œç»™å‡ºä¸€ä¸ªâ€œå¿ å® + å·¥ç¨‹å‹å¥½â€çš„ PyTorch å®ç°ï¼š
#   - MRLPF_SpatialResidualBlock:    R(Y)
#   - MRLPF_FrequencyResidualBlock:  F_freq(Y)
#   - MRLPF_LowPassAttention:        self-attention ä½é€š + depthwise conv
#   - MRLPFBlock / MultiScaleResidualLowPassFilterBlock: ç»„åˆæˆå®Œæ•´æ¨¡å—
#
# è¾“å…¥ / è¾“å‡º: (B, C, H, W) -> (B, C, H, W)
# ============================================================


# ------------------------------------------------------------
# 1. Spatial Residual Branch  R(Y)
#    ä¸¤ä¸ª 3x3 å·ç§¯ + ReLUï¼Œå¯¹åº”è®ºæ–‡é‡Œçš„ Conv3 + ReLU + Conv3
# ------------------------------------------------------------

class MRLPF_SpatialResidualBlock(nn.Module):
    """
    Spatial Residual Block for MRLPF:

        R(Y) = Conv3x3(ReLU(Conv3x3(Y)))

    è¾“å…¥ / è¾“å‡º: (B, C, H, W) -> (B, C, H, W)
    """

    def __init__(self, channels: int):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(channels)
        self.act = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self.act(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        return out


# ------------------------------------------------------------
# 2. Frequency Residual Branch  F_freq(Y)
#    å‚è€ƒ DeepRFT / è®ºæ–‡ [15] çš„åšæ³•ï¼š
#    - 2D FFT â†’ å®éƒ¨/è™šéƒ¨åœ¨é€šé“ç»´æ‹¼æ¥
#    - Conv1x1 + ReLU + Conv1x1
#    - å† iFFT å›ç©ºé—´åŸŸï¼Œå¾—åˆ° F_freq(Y)
# ------------------------------------------------------------

class MRLPF_FrequencyResidualBlock(nn.Module):
    """
    Frequency Residual Block for MRLPF:

        F_freq(Y) = iFFT( Conv1x1( ReLU( Conv1x1( FFT(Y) ) ) ) )

    è¿™é‡Œ FFT ä½¿ç”¨ torch.fft.fft2 / ifft2ï¼Œ
    å®éƒ¨å’Œè™šéƒ¨åœ¨é€šé“ç»´æ‹¼æ¥ï¼Œç”¨ Conv1x1 è¿›è¡Œçº¿æ€§å˜æ¢ã€‚

    è¾“å…¥ / è¾“å‡º: (B, C, H, W) -> (B, C, H, W)
    """

    def __init__(self, channels: int):
        super().__init__()
        self.channels = channels
        self.conv1 = nn.Conv2d(2 * channels, 2 * channels, kernel_size=1, bias=True)
        self.conv2 = nn.Conv2d(2 * channels, 2 * channels, kernel_size=1, bias=True)
        self.act = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C, H, W = x.shape
        assert C == self.channels

        # 2D FFT
        spec = torch.fft.fft2(x, norm="ortho")  # complex: (B, C, H, W)
        real = spec.real
        imag = spec.imag

        feat = torch.cat([real, imag], dim=1)  # (B, 2C, H, W)
        feat = self.act(self.conv1(feat))
        feat = self.conv2(feat)

        real2, imag2 = torch.chunk(feat, 2, dim=1)
        spec_new = torch.complex(real2, imag2)

        # iFFT å›åˆ°ç©ºé—´åŸŸï¼Œå–å®éƒ¨
        out = torch.fft.ifft2(spec_new, norm="ortho").real  # (B, C, H, W)
        return out


# ------------------------------------------------------------
# 3. Learnable Low-Pass Filter via Self-Attention
#    MRLPF_LowPassAttention å¯¹åº” Eqs. (4)-(5)ï¼š
#
#      H_i = sum_j S_ij f_j,   S = softmax(QK^T / sqrt(d))
#      E = F + D(H)
#
#    è¿™é‡Œå®ç°ä¸ºæ ‡å‡† scaled dot-product self-attention +
#    ä¸€ä¸ª 3x3 depthwise conv ä½œä¸º D(Â·)ã€‚
# ------------------------------------------------------------

class MRLPF_LowPassAttention(nn.Module):
    """
    Learnable Low-Pass Filter via Self-Attention.

    è¾“å…¥:  F âˆˆ R^{BÃ—CÃ—HÃ—W}
    è¾“å‡º:  E âˆˆ R^{BÃ—CÃ—HÃ—W}

    æ­¥éª¤:
      1. æŠŠ F è§†ä¸º N=H*W ä¸ª tokenï¼Œç»´åº¦ C
      2. åŸºäº F åš Q,K,V (çº¿æ€§å±‚)ï¼Œè‡ªæ³¨æ„åŠ›å¾—åˆ° H (ä½é€šç‰¹å¾)
      3. H reshape å› (B,C,H,W)ï¼Œè¿‡ depthwise Conv3x3 â†’ D(H)
      4. E = F + D(H)
    """

    def __init__(self, channels: int, num_heads: int = 4):
        super().__init__()
        assert channels % num_heads == 0, "channels must be divisible by num_heads"
        self.channels = channels
        self.num_heads = num_heads
        self.head_dim = channels // num_heads

        self.q_proj = nn.Linear(channels, channels)
        self.k_proj = nn.Linear(channels, channels)
        self.v_proj = nn.Linear(channels, channels)
        self.out_proj = nn.Linear(channels, channels)

        # D(Â·): depthwise Conv3x3
        self.dw = nn.Conv2d(
            channels, channels, kernel_size=3, padding=1, groups=channels, bias=False
        )
        self.dw_bn = nn.BatchNorm2d(channels)

    def forward(self, F: torch.Tensor) -> torch.Tensor:
        B, C, H, W = F.shape
        N = H * W

        # reshape to (B, N, C)
        x = F.view(B, C, N).permute(0, 2, 1)  # (B, N, C)

        # Q,K,V: (B, N, C)
        Q = self.q_proj(x)
        K = self.k_proj(x)
        V = self.v_proj(x)

        # å¤šå¤´æ‹†åˆ†: (B, num_heads, N, head_dim)
        def split_heads(t):
            return t.view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)

        Qh = split_heads(Q)
        Kh = split_heads(K)
        Vh = split_heads(V)

        # Attention: (B, num_heads, N, N)
        attn = torch.matmul(Qh, Kh.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = torch.softmax(attn, dim=-1)

        # H: (B, num_heads, N, head_dim)
        Hh = torch.matmul(attn, Vh)

        # åˆå¹¶å¤´: (B, N, C)
        H_flat = Hh.permute(0, 2, 1, 3).contiguous().view(B, N, C)
        H_flat = self.out_proj(H_flat)  # (B, N, C)

        # reshape å› (B, C, H, W)
        H_feat = H_flat.permute(0, 2, 1).contiguous().view(B, C, H, W)

        # D(H) depthwise conv
        DH = self.dw_bn(self.dw(H_feat))

        # æ®‹å·®
        E = F + DH
        return E


# ------------------------------------------------------------
# 4. MRLPF Block (Residual Low-Pass Filter Block)
#    æœ€ç»ˆç»„åˆæ¨¡å—ï¼Œå¯¹åº”è®ºæ–‡é‡Œçš„ RLPF æ¨¡å—ã€‚
#
#    F = Y + R(Y) + F_freq(Y)
#    E = LowPassAttention(F)
# ------------------------------------------------------------

class MRLPFBlock(nn.Module):
    """
    MRLPF Block (Residual Low-Pass Filter Block).

    è¾“å…¥ / è¾“å‡º: (B, C, H, W) -> (B, C, H, W)

    ä½ å¯ä»¥æŠŠå®ƒç›´æ¥å½“æˆ:
      - ResNet Block çš„æ›¿ä»£
      - UNet bottleneck / encoder / decoder block çš„æ›¿ä»£
      - EIT / CT / Deblur ç­‰é‡å»ºç½‘ç»œé‡Œçš„â€œä½é€šå…ˆéªŒâ€æ¨¡å—
    """

    def __init__(
        self,
        channels: int,
        num_heads: int = 4,
    ):
        super().__init__()
        self.spatial_branch = MRLPF_SpatialResidualBlock(channels)
        self.freq_branch = MRLPF_FrequencyResidualBlock(channels)
        self.lpf = MRLPF_LowPassAttention(channels, num_heads=num_heads)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Eq. (3): F = Y + R(Y) + F_freq(Y)
        spatial_res = self.spatial_branch(x)
        freq_res = self.freq_branch(x)
        F = x + spatial_res + freq_res

        # Eq. (4)(5): learnable low-pass filter via self-attention
        E = self.lpf(F)
        return E


# å…¼å®¹ä½ åœ¨å¤–é¢å¼•ç”¨æ—¶å–œæ¬¢ç”¨çš„é•¿åå­—
class MultiScaleResidualLowPassFilterBlock(MRLPFBlock):
    """
    åˆ«åï¼šMultiScaleResidualLowPassFilterBlock
    å®é™…ä¸Šå°±æ˜¯ MRLPFBlockï¼Œæœ¬èº«ä¸åšå¤šå°ºåº¦ä¸‹é‡‡æ ·ï¼Œ
    â€œmulti-scaleâ€ æ˜¯æ•´ä¸ª MRLPFNet æ¡†æ¶çš„ coarse-to-fine ç»“æ„å¸¦æ¥çš„ã€‚
    """
    pass


# ============================================================
# 5. æµ‹è¯•è„šæœ¬
#    - Forward shape æ£€æŸ¥
#    - NNI ç»Ÿè®¡ FLOPs / Paramsï¼ˆå’Œä½  EcNet å°è„šæœ¬åŒé£æ ¼ï¼‰
# ============================================================

def build_test_module(name: str):
    """
    æ”¯æŒæµ‹è¯•:
      - "spatial"  : MRLPF_SpatialResidualBlock
      - "freq"     : MRLPF_FrequencyResidualBlock
      - "lpf"      : MRLPF_LowPassAttention
      - "mrlpf"    : MRLPFBlock
      - "msrlpf"   : MultiScaleResidualLowPassFilterBlock
    """
    name = name.lower()
    C = 32
    H = W = 32
    x = torch.rand(1, C, H, W)

    if name == "spatial":
        module = MRLPF_SpatialResidualBlock(channels=C)
        inputs = (x,)
    elif name == "freq":
        module = MRLPF_FrequencyResidualBlock(channels=C)
        inputs = (x,)
    elif name == "lpf":
        module = MRLPF_LowPassAttention(channels=C, num_heads=4)
        inputs = (x,)
    elif name == "mrlpf":
        module = MRLPFBlock(channels=C, num_heads=4)
        inputs = (x,)
    elif name == "msrlpf":
        module = MultiScaleResidualLowPassFilterBlock(channels=C, num_heads=4)
        inputs = (x,)
    else:
        raise ValueError(f"Unknown module name: {name}")

    return module, inputs


if __name__ == "__main__":
    # è¿™é‡Œæ”¹åå­—å°±èƒ½æµ‹ä¸åŒæ¨¡å—:
    # "spatial", "freq", "lpf", "mrlpf", "msrlpf"
    module_name = "mrlpf"

    model, inputs = build_test_module(module_name)

    print(f"ğŸ”§ Testing MRLPF module: {module_name}")
    in_shapes = ", ".join(str(t.shape) for t in inputs)

    # --- Forward æµ‹è¯• ---
    try:
        with torch.no_grad():
            out = model(*inputs)
        if isinstance(out, tuple):
            out_shapes = ", ".join(str(t.shape) for t in out)
        else:
            out_shapes = str(out.shape)
        print(f"âœ… Forward Pass Success: {in_shapes} â†’ {out_shapes}")
    except Exception as e:
        print(f"âŒ Forward Failed: {e}")

    # --- FLOPs / Params ---
    try:
        from nni.compression.utils.counter import count_flops_params

        flops, params, _ = count_flops_params(model, x=inputs)
        print(f"ğŸ“Š FLOPs:  {flops / 1e6:.2f} MFLOPs | Params: {params / 1e6:.4f} M")
    except ImportError:
        print("âš ï¸ NNI not installed. Run: pip install nni")
    except Exception as e:
        print(f"âš ï¸ FLOPs/Params counting failed: {e}")
