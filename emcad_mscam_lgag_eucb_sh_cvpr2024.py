import torch
import torch.nn as nn
import torch.nn.functional as F


# ============================================================
# EMCAD: Efficient Multi-scale Convolutional Attention Decoding
#   CVPR 2024
#   Paper: "EMCAD: Efficient Multi-scale Convolutional Attention Decoding
#           for Medical Image Segmentation"
#   Official repo: https://github.com/SLDGroup/EMCAD
#
# è¿™é‡Œæ•´ç†çš„æ˜¯å‡ ä¸ªâ€œå¯æŠ å‡ºæ¥å•ç‹¬ç”¨â€çš„å°æ¨¡å—ï¼ˆç®€åŒ–å®ç°ç‰ˆæœ¬ï¼‰ï¼š
#   - MSCB: Multi-scale Convolution Block
#   - MSCAM: Multi-scale Convolutional Attention Module
#   - LGAG: Large-kernel Grouped Attention Gate
#   - EUCB: Efficient Up-Convolution Block
#   - SegHead: 1x1 segmentation head
#
# å…¨éƒ¨éƒ½æŒ‰ NCHW (B, C, H, W) å†™ï¼Œæ–¹ä¾¿æ’åˆ° UNet / EcNet / PVT ç­‰ç»“æ„é‡Œã€‚
# ============================================================


# -----------------------------
# å°å·¥å…·: channel shuffle
# -----------------------------
def channel_shuffle(x: torch.Tensor, groups: int) -> torch.Tensor:
    """
    æ ‡å‡†çš„ channel shuffle æ“ä½œï¼š
    æŠŠé€šé“åˆ†ç»„åé‡æ–°æ‰“ä¹±ï¼Œå¢å¼ºç»„é—´ä¿¡æ¯äº¤äº’ã€‚
    """
    b, c, h, w = x.size()
    assert c % groups == 0, "channels must be divisible by groups"
    x = x.view(b, groups, c // groups, h, w)
    x = x.permute(0, 2, 1, 3, 4).contiguous()
    x = x.view(b, c, h, w)
    return x


# ============================================================
# 1. Multi-scale Convolution Block (MSCB)
#    - å‚è€ƒ EMCAD è®ºæ–‡ä¸­ MSCB çš„è®¾è®¡æ€è·¯:
#      inverted residual (æ‰©å¼  -> å¤šå°ºåº¦ depthwise -> å‹ç¼© + é€šé“ shuffle)
# ============================================================

class EMCAD_MSCB(nn.Module):
    """
    Multi-scale Convolution Block (ç®€åŒ–ç‰ˆ).

    è¾“å…¥ / è¾“å‡º: (B, C, H, W) -> (B, C, H, W)
    - å…ˆç”¨ 1x1 å·ç§¯æ‰©å±•é€šé“ (factor=2)
    - å¤šå°ºåº¦ depthwise conv é¡ºåºå †å ï¼Œå¹¶ç”¨æ®‹å·®å½¢å¼ç´¯è®¡
    - é€šé“ shuffle ä¿ƒè¿›ç»„é—´ä¿¡æ¯äº¤äº’
    - 1x1 å·ç§¯å‹å›åŸå§‹é€šé“æ•°ï¼Œå¹¶åŠ ä¸Š input æ®‹å·®
    """

    def __init__(
        self,
        dim: int,
        expansion: int = 2,
        kernel_sizes=(3, 5, 7),
        shuffle_groups: int = 4,
    ):
        super().__init__()
        self.dim = dim
        self.expanded_dim = dim * expansion
        self.kernel_sizes = kernel_sizes
        self.shuffle_groups = shuffle_groups

        # 1x1 conv: æ‰©å±•é€šé“
        self.expand = nn.Sequential(
            nn.Conv2d(dim, self.expanded_dim, kernel_size=1, bias=False),
            nn.BatchNorm2d(self.expanded_dim),
            nn.ReLU6(inplace=True),
        )

        # å¤šå°ºåº¦ depthwise conv åºåˆ—
        dw_layers = []
        for k in kernel_sizes:
            p = k // 2
            dw_layers.append(
                nn.Sequential(
                    nn.Conv2d(
                        self.expanded_dim,
                        self.expanded_dim,
                        kernel_size=k,
                        padding=p,
                        groups=self.expanded_dim,
                        bias=False,
                    ),
                    nn.BatchNorm2d(self.expanded_dim),
                    nn.ReLU6(inplace=True),
                )
            )
        self.ms_dw = nn.ModuleList(dw_layers)

        # å‹å›åŸå§‹é€šé“
        self.project = nn.Sequential(
            nn.Conv2d(self.expanded_dim, dim, kernel_size=1, bias=False),
            nn.BatchNorm2d(dim),
        )

        self.act = nn.ReLU6(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, C, H, W)
        identity = x

        out = self.expand(x)  # (B, 2C, H, W)

        # é¡ºåºå †å å¤šå°ºåº¦ depthwise convï¼Œæ¯å±‚éƒ½æœ‰æ®‹å·®
        for dw in self.ms_dw:
            out = out + dw(out)

        # é€šé“ shuffle
        if self.shuffle_groups > 1:
            out = channel_shuffle(out, self.shuffle_groups)

        # å‹å›åŸé€šé“å¹¶åŠ ä¸Š identity
        out = self.project(out)
        if out.shape == identity.shape:
            out = out + identity

        out = self.act(out)
        return out


# ============================================================
# 2. Channel Attention Block (CAB) & Spatial Attention Block (SAB)
#    - ä½¿ç”¨ç±»ä¼¼ SE / CBAM çš„ç»å…¸å®ç°ï¼Œè®ºæ–‡ä¸­ä¹Ÿæ˜¯é€šé“+ç©ºé—´æ³¨æ„åŠ›ç»„åˆ
# ============================================================

class EMCAD_CAB(nn.Module):
    """
    Channel Attention Block (CAB) - ç±»ä¼¼ SE, ç”¨ GAP + MLP åšé€šé“æ³¨æ„åŠ›ã€‚
    """

    def __init__(self, dim: int, reduction: int = 16):
        super().__init__()
        hidden = max(dim // reduction, 4)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.mlp = nn.Sequential(
            nn.Conv2d(dim, hidden, kernel_size=1, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden, dim, kernel_size=1, bias=True),
            nn.Sigmoid(),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w = self.avg_pool(x)
        w = self.mlp(w)
        return x * w


class EMCAD_SAB(nn.Module):
    """
    Spatial Attention Block (SAB) - ç±»ä¼¼ CBAM çš„ç©ºé—´æ³¨æ„åŠ›ï¼Œ
    ç”¨ avg_pool + max_pool çš„æ‹¼æ¥å†åš 7x7 convã€‚
    """

    def __init__(self, kernel_size: int = 7):
        super().__init__()
        padding = kernel_size // 2
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # é€šé“ç»´åš avg / max pooling
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        s = torch.cat([avg_out, max_out], dim=1)  # (B, 2, H, W)
        s = self.conv(s)
        s = self.sigmoid(s)
        return x * s


# ============================================================
# 3. Multi-scale Convolutional Attention Module (MSCAM)
#    - CAB + SAB + MSCB
# ============================================================

class EMCAD_MSCAM(nn.Module):
    """
    Multi-scale Convolutional Attention Module.

    è¾“å…¥ / è¾“å‡º: (B, C, H, W) -> (B, C, H, W)
    MSCAM(x) = MSCB(SAB(CAB(x)))ï¼Œå¤–é¢å†åŠ ä¸€æ¬¡ residualã€‚
    """

    def __init__(
        self,
        dim: int,
        expansion: int = 2,
        kernel_sizes=(3, 5, 7),
        shuffle_groups: int = 4,
        reduction: int = 16,
    ):
        super().__init__()
        self.cab = EMCAD_CAB(dim, reduction=reduction)
        self.sab = EMCAD_SAB(kernel_size=7)
        self.mscb = EMCAD_MSCB(
            dim=dim,
            expansion=expansion,
            kernel_sizes=kernel_sizes,
            shuffle_groups=shuffle_groups,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        identity = x
        out = self.cab(x)
        out = self.sab(out)
        out = self.mscb(out)
        # å†åŠ ä¸€å±‚çŸ­æ®‹å·®ï¼Œç•¥å¾®è´´è¿‘è®ºæ–‡â€œrefinementâ€çš„æ„Ÿè§‰
        if out.shape == identity.shape:
            out = out + identity
        return out


# ============================================================
# 4. Large-kernel Grouped Attention Gate (LGAG)
#    - æ¥è‡ª EMCAD è§£ç å™¨ä¸­çš„å¤§æ ¸åˆ†ç»„æ³¨æ„åŠ›é—¨ï¼š
#      ç”¨ 3x3 group conv åˆ†åˆ«å¤„ç† g (skip) å’Œ x (up-sampled)ï¼Œ
#      å† 1x1 conv + Sigmoid å¾—åˆ°å•é€šé“ gateï¼Œå¯¹ x åšç¼©æ”¾ã€‚
# ============================================================

class EMCAD_LGAG(nn.Module):
    """
    Large-kernel Grouped Attention Gate (LGAG).

    Args:
        channels: g å’Œ x çš„é€šé“æ•° (å‡è®¾å·²ç» match å¥½)
        groups:   group conv çš„ç»„æ•°

    Inputs:
        g: æ¥è‡ª skip connection çš„ç‰¹å¾ (B, C, H, W)
        x: ä¸Šé‡‡æ ·åçš„å½“å‰ stage ç‰¹å¾ (B, C, H, W)

    Output:
        gated x: (B, C, H, W)
    """

    def __init__(self, channels: int, groups: int = 4):
        super().__init__()
        self.gc_g = nn.Conv2d(
            channels, channels, kernel_size=3, padding=1, groups=groups, bias=False
        )
        self.gc_x = nn.Conv2d(
            channels, channels, kernel_size=3, padding=1, groups=groups, bias=False
        )
        self.bn_g = nn.BatchNorm2d(channels)
        self.bn_x = nn.BatchNorm2d(channels)

        self.relu = nn.ReLU(inplace=True)

        self.conv1x1 = nn.Conv2d(channels, 1, kernel_size=1, bias=True)
        self.bn_out = nn.BatchNorm2d(1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, g: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
        # g, x : (B, C, H, W)ï¼Œå‡è®¾å·²ç»åŒåˆ†è¾¨ç‡
        g_feat = self.bn_g(self.gc_g(g))
        x_feat = self.bn_x(self.gc_x(x))

        h = self.relu(g_feat + x_feat)
        att = self.conv1x1(h)
        att = self.bn_out(att)
        att = self.sigmoid(att)  # (B, 1, H, W)

        out = x * att
        return out


# ============================================================
# 5. Efficient Up-Convolution Block (EUCB)
#    - ä¸Šé‡‡æ ·æ¨¡å—: UpSampling -> depthwise 3x3 -> 1x1 conv
# ============================================================

class EMCAD_EUCB(nn.Module):
    """
    Efficient Up-Convolution Block (EUCB).

    è¾“å…¥ / è¾“å‡º:
        è¾“å…¥: (B, C_in, H, W)
        è¾“å‡º: (B, C_out, 2H, 2W)
    """

    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)

        self.dw = nn.Conv2d(
            in_channels,
            in_channels,
            kernel_size=3,
            padding=1,
            groups=in_channels,
            bias=False,
        )
        self.dw_bn = nn.BatchNorm2d(in_channels)
        self.dw_act = nn.ReLU(inplace=True)

        self.pw = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.pw_bn = nn.BatchNorm2d(out_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.up(x)
        x = self.dw_act(self.dw_bn(self.dw(x)))
        x = self.pw_bn(self.pw(x))
        return x


# ============================================================
# 6. Segmentation Head (SH)
#    - éå¸¸ç®€å•: 1x1 å·ç§¯æŠŠé€šé“æ•°æ˜ å°„åˆ°ç±»åˆ«æ•°
# ============================================================

class EMCAD_SegHead(nn.Module):
    """
    Segmentation Head (SH).

    è¾“å…¥: (B, C_in, H, W)
    è¾“å‡º: (B, num_classes, H, W)
    """

    def __init__(self, in_channels: int, num_classes: int = 1):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1, bias=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.conv(x)


# ============================================================
# 7. æµ‹è¯•è„šæœ¬ (å’Œä½  EcNet çš„é£æ ¼ä¿æŒä¸€è‡´)
#    - Forward shape æµ‹è¯•
#    - NNI ç»Ÿè®¡ FLOPs / Params (å¯é€‰)
# ============================================================

def build_test_module(name: str):
    """
    æ ¹æ®åå­—æ„é€ ä¸€ä¸ªå¾…æµ‹è¯•æ¨¡å— + è¾“å…¥å¼ é‡ã€‚
    """
    name = name.lower()

    if name == "mscb":
        dim = 16
        module = EMCAD_MSCB(dim=dim)
        inputs = (torch.rand(1, dim, 32, 32),)  # x

    elif name == "mscam":
        dim = 16
        module = EMCAD_MSCAM(dim=dim)
        inputs = (torch.rand(1, dim, 32, 32),)  # x

    elif name == "lgag":
        dim = 16
        module = EMCAD_LGAG(channels=dim)
        g = torch.rand(1, dim, 32, 32)
        x = torch.rand(1, dim, 32, 32)
        inputs = (g, x)

    elif name == "eucb":
        cin, cout = 16, 8
        module = EMCAD_EUCB(in_channels=cin, out_channels=cout)
        inputs = (torch.rand(1, cin, 32, 32),)  # x

    elif name == "sh":
        cin = 16
        module = EMCAD_SegHead(in_channels=cin, num_classes=2)
        inputs = (torch.rand(1, cin, 32, 32),)  # x

    else:
        raise ValueError(f"Unknown module name: {name}")

    return module, inputs


if __name__ == "__main__":
    # è¿™é‡Œåˆ‡æ¢è¦æµ‹è¯•çš„æ¨¡å—åå­—:
    # å¯é€‰: "mscb", "mscam", "lgag", "eucb", "sh"
    module_name = "eucb"

    model, inputs = build_test_module(module_name)

    print(f"ğŸ”§ Testing EMCAD module: {module_name}")

    # --- Forward æµ‹è¯• ---
    try:
        with torch.no_grad():
            out = model(*inputs)
        in_shapes = ", ".join([str(t.shape) for t in inputs])
        print(f"âœ… Forward Pass Success: {in_shapes} â†’ {tuple(out.shape)}")
    except Exception as e:
        print(f"âŒ Forward Failed: {e}")

    # --- FLOPs / Params ---
    try:
        from nni.compression.utils.counter import count_flops_params

        flops, params, _ = count_flops_params(model, x=inputs)
        print(f"ğŸ“Š FLOPs:  {flops / 1e6:.2f} MFLOPs | Params: {params / 1e6:.4f} M")
    except ImportError:
        print("âš ï¸ NNI not installed. Run: pip install nni")
    except Exception as e:
        print(f"âš ï¸ FLOPs/Params counting failed: {e}")
